# Silas V2 - Hierarchical RAG Agent

## Concept

Silas V2 utilise le pattern **Hierarchical RAG**.
Au lieu d'analyser des fichiers entiers, on dÃ©coupe en **chunks** pour une prÃ©cision chirurgicale.

## DiffÃ©rence V1 vs V2

| Aspect | V1 | V2 |
|--------|----|----|
| UnitÃ© de traitement | Fichier entier | Chunk (segment) |
| SÃ©lection | Fichiers pertinents | Chunks pertinents |
| PrÃ©cision | Moyenne (bruit) | Haute (ciblÃ©e) |
| Gros documents | ProblÃ©matique | OptimisÃ© |

## Architecture

```
silas-v2/
â”œâ”€â”€ config.py      # Configuration (modÃ¨les + chunking)
â”œâ”€â”€ chunker.py     # DÃ©coupage intelligent des documents
â”œâ”€â”€ prompts.py     # Prompts systÃ¨me
â”œâ”€â”€ rag_graph.py   # Pipeline LangGraph
â”œâ”€â”€ main.py        # Point d'entrÃ©e CLI
â””â”€â”€ llm.md         # Cette documentation
```

## Flux de traitement (5 Ã©tapes)

```
[Dossier de docs]
       â†“
   â‘  CHUNKER (Code Python - pas de LLM)
       â†’ DÃ©coupe chaque doc en segments
       â†’ Respecte les frontiÃ¨res naturelles (paragraphes, phrases)
       â†’ GÃ©nÃ¨re: doc1_s1, doc1_s2, doc2_s1...
       â†“
   â‘¡ SCANNER (FAST_MODEL: qwen3:0.6b)
       â†’ RÃ©sume chaque chunk en 1 phrase
       â†’ Traitement parallÃ¨le possible
       â†“
   â‘¢ SELECTOR (CHOOSE_MODEL: qwen3:8b)
       â†’ ReÃ§oit: question + tous les rÃ©sumÃ©s
       â†’ Retourne: ["doc1_s3", "doc2_s7"] (chunks pertinents)
       â†“
   â‘£ EXTRACTOR (SMART_MODEL: qwen3:14b)
       â†’ Lit UNIQUEMENT les chunks sÃ©lectionnÃ©s
       â†’ Extrait les passages qui rÃ©pondent
       â†“
   â‘¤ SYNTHESIZER (SMART_MODEL: qwen3:14b)
       â†’ RÃ©dige la rÃ©ponse finale avec citations
       â†“
[RÃ©ponse avec [chunk_id : evidence]]
```

## Chunking Intelligent

Le chunker (`chunker.py`) dÃ©coupe sans LLM en utilisant des heuristiques:

### ParamÃ¨tres (config.py)

- `CHUNK_SIZE = 1500` : Taille cible en caractÃ¨res
- `CHUNK_OVERLAP = 200` : Chevauchement pour prÃ©server le contexte
- `MIN_CHUNK_SIZE = 300` : Ã‰vite les micro-chunks inutiles

### SÃ©parateurs (ordre de prioritÃ©)

1. `\n\n\n` - Triple saut (section majeure)
2. `\n\n` - Double saut (paragraphe)
3. `\n` - Simple saut (ligne)
4. `. ` - Fin de phrase
5. `, ` - Virgule
6. ` ` - Espace (dernier recours)

### Exemple de dÃ©coupage

```
Document: rapport.md (4500 chars)
â†’ rapport.md_s1 (1450 chars) - Introduction
â†’ rapport.md_s2 (1500 chars) - Analyse
â†’ rapport.md_s3 (1550 chars) - Conclusions
```

## Les 3 modÃ¨les

| ModÃ¨le | Alias | TÃ¢che | Justification |
|--------|-------|-------|---------------|
| `qwen3:0.6b` | FAST_MODEL | Scan/rÃ©sumÃ© | Ultra-rapide, traite des dizaines de chunks |
| `qwen3:8b` | CHOOSE_MODEL | SÃ©lection logique | Bon raisonnement, coÃ»t modÃ©rÃ© |
| `qwen3:14b` | SMART_MODEL | Extraction + rÃ©daction | PrÃ©cision maximale |

## Structure de l'Ã©tat (StateGraph)

```python
class AgentState(TypedDict):
    question: str                   # Question utilisateur
    file_directory: str             # Chemin du dossier
    chunks: List[dict]              # Chunks sÃ©rialisÃ©s
    chunk_summaries: List[str]      # ["chunk_id: rÃ©sumÃ©", ...]
    selected_chunks: List[str]      # ["doc1_s3", "doc2_s7"]
    extracted_evidence: List[str]   # Passages extraits
    final_answer: str               # RÃ©ponse finale
    timings: dict                   # Temps par Ã©tape
```

## Utilisation CLI

```bash
# Usage basique
python main.py -q "Quelle est la conclusion du rapport?" -d ./documents/

# Mode verbeux
python main.py -q "RÃ©sume les points clÃ©s" -d ./projet/ -v
```

## Sortie console

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                   SILAS V2 - Hierarchical RAG                 â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â”Œâ”€ Configuration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ‡ FAST Model : qwen3:0.6b                                â”‚
â”‚  âš–ï¸  CHOOSE     : qwen3:8b                                  â”‚
â”‚  ğŸ§  SMART      : qwen3:14b                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

============================================================
âœ‚ï¸ Ã‰TAPE 1: CHUNKING
============================================================
  â€¢ Fichiers traitÃ©s: 3
  â€¢ Chunks gÃ©nÃ©rÃ©s: 12

============================================================
ğŸ” Ã‰TAPE 2: SCAN
============================================================
  [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 12/12 (100%)

... (autres Ã©tapes)

â”Œâ”€ Temps d'exÃ©cution â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Chunking     â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘   0.02s ( 0.5%)        â”‚
â”‚  Scanning     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘   3.45s (35.2%)        â”‚
â”‚  Selection    â–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘   0.89s ( 9.1%)        â”‚
â”‚  Extraction   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘   4.21s (43.0%)        â”‚
â”‚  Synthesis    â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘   1.19s (12.2%)        â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”‚
â”‚  TOTAL                               9.76s                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Points d'attention

### Modifier le chunking
- `CHUNK_SIZE` dans `config.py` pour ajuster la granularitÃ©
- `CHUNK_OVERLAP` pour plus/moins de contexte partagÃ©

### Changer un modÃ¨le
- Modifier `FAST_MODEL`, `CHOOSE_MODEL` ou `SMART_MODEL` dans `config.py`

### Ajouter une extension
- Modifier `SUPPORTED_EXTENSIONS` dans `config.py`

### Debug du chunking
```bash
python chunker.py ./mon_dossier/
```

## Optimisations possibles

### Fusion Scanner + Selector
Si le CHOOSE_MODEL est assez bon, on peut fusionner les Ã©tapes 2 et 3:
```
"Voici 12 chunks. Lesquels rÃ©pondent Ã  : {question} ?"
â†’ ["doc1_s3", "doc2_s7"]
```
Activer via `SELECTOR_DIRECT_PROMPT` dans `prompts.py`.

### ParallÃ©lisation
Le scan (Ã©tape 2) peut Ãªtre parallÃ©lisÃ© avec `asyncio` pour les gros corpus.

### Cache des rÃ©sumÃ©s
Stocker les rÃ©sumÃ©s de chunks dÃ©jÃ  scannÃ©s pour Ã©viter de les recalculer.

## DÃ©pendances

```
langchain-ollama
langgraph
```

Ollama doit tourner sur `http://localhost:11434`.
